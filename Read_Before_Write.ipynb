{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ramya_Banda_Read_Before_Write.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "566Gji6z6ojT",
        "colab_type": "text"
      },
      "source": [
        "#Name: Ramya Banda\n",
        "#ECE 595 Machine Learning II\n",
        "#Project 5: NTM Part 2 - Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v_hJdz1B4bd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b5f00e6a-b671-4af7-ed47-a8827e6dc90e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan  8 21:20:45 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sWni0FseVUz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "416c1da1-6270-4cbc-8823-879c0ef19272"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\", force_remount=True)\n",
        "#%cd gdrive/My Drive/Neural_Turing_Machine/NTM_small\n",
        "%cd gdrive/My Drive/NTM_Student"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/My Drive/NTM_Student\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9gdekJg_-xa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "50ad0f2c-5532-4fe0-c614-2072fcc3168e"
      },
      "source": [
        "from utils import OmniglotDataLoader, one_hot_decode, five_hot_decode\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efHLdzvkAKfK",
        "colab_type": "text"
      },
      "source": [
        "The following class `MANNCell` is the core of the memory-augmented neural network (MANN). You will implement the main parts of it in Tensorflow 2.0.\n",
        "\n",
        "Before any technical discussion of how the MANNCell should operate, let us look at what it should do on a general level. Suppose we have an input batch of 16 episodes of image samples, with each episode being of equal length of 50. Based on the design of the rest of the project (which we have already implemented for you), MANNCell should be called 50 times, each time having 16 input samples (along with the offseted labels), and outputting 16 output labels. More specifically, the MANNCell should produce classification labels $[\\hat{y}_0^t, ..., \\hat{y}_{15}^t]$ for all 16 iteration-$t$ image samples batch $[x^t_0+\\text{null}, x^t_1+y_0^t, ..., x^t_{15}+y_{14}^t]$ (\"+\" means concatenation) every time it is called; for your information, it is the class NTMOneShotLearningModel (already implemented below) that actually calls MANNCell 50 times. Your job is to make sure that at a single iteration $t$ (where $t=0,1,2,...,49$), MANNCell correctly parses the input arguments, produce the correct read and write weights $w^r_t, w^w_t$, correctly retrieve from and write to the memory to form $M_t$, and use the right material to get the logits for classification (they will be used for computing the labels and cross-entropy values in NTMOneShotLearningModel), and return the right states that will be used in the next iteration $t+1$. \n",
        "\n",
        "Let us look at the input arguments of the method `call(self, inputs, states)`  of this class first:\n",
        "*   The `inputs` variable shall have the following shape: \n",
        "    `(batch_size, image_size+num_classes)`. \n",
        "  *   It corresponds to the $[x^t_0+\\text{null}, x^t_1+y^t_0, ..., x^t_{15}+y^t_{14}]$ above, for some iteration $t=0,1,...,49$.\n",
        "  *   `inputs[p,:]` is the $p$-th image in the batch `inputs` (note that the images are flattened to 1D tensors, and the labels are one-hot encoded).\n",
        "*   The `states` variable is a dictionary that has the following set of keys:`{'controller_state', 'read_vector_list', 'w_r_list', 'w_u', 'M'}`\n",
        "  *   `controller_state` is the state of the controller in iteration $t-1$; if $t-1 < 0$, then it is just zero-filled. As it is an LSTM cell, `controller_state` is of the form `[(batch_size, rnn_size),(batch_size, rnn_size)]` (technically speaking its shape is `(2, batch_size, rnn_size)`). The two `(batch_size, rnn_size)`-shaped entries in it correspond to the cell state and the hidden state of the LSTM. We will mostly be treating the LSTM controller as a black-box in this project, so we do not need to pay much attention to the details of its states. If interested, you can read about the LSTM cell's technical details in [tf.keras.layers.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell).\n",
        "  *   `read_vector_list` is the list of read vectors $r_{t-1}$ which we obtained in the previous iteration $t-1$ in the episode; if $t-1 < 0$, then the read vector list is initialized to be an arbitrary one-hot vector. It is of the shape `(head_num, batch_size, memory_vector_dim)`. Basically, `read_vector_list[i,p,:]` is the $(t-1)$-th-iteration read vector of the $i$-th read head for the $p$-th input sample in the batch.\n",
        "  *   `w_r_list` is the list of read weights $w^r_{t-1}$ which we obtained in the previous iteration $t-1$ in the episode; if $t-1 < 0$, then the read weights list is initialized to be an arbitrary one-hot vector. It is of the shape `(head_num, batch_size, num_memory_slots)`. Basically, `w_r_list[i,p,:]` is the $(t-1)$-th-iteration read weight of the $i$-th read head for the $p$-th input sample in the batch.\n",
        "  *   `w_u` is the list of memory usage weights $w^u_{t-1}$ which we obtained in the previous iteration $t-1$ in the episode; if $t-1 < 0$, then the usage weights list is is initialized to be an arbitrary one-hot vector. It is of the shape `(batch_size, num_memory_slots)`. Basically, `w_u[p,:]` is the $(t-1)$-th-iteration memory usage weight of the $p$-th input sample in the batch.\n",
        "  *   `M` is the memory content from the previous iteration $t-1$; if $t-1 < 0$, then the memory is just zero-filled. It is of shape `(batch_size, num_memory_slots, memory_vector_dim)`. Basically, `M[p,j,:]` is the $j$-th memory vector in the memory block for the $p$-th sample in the batch from iteration $t-1$, and `M[p,:,:]` is the memory block for the $p$-th sample in the batch, where the memory block is a 2D structure that has `num_memory_slots` memory vectors, each vector of length `memory_vector_dim`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivOh5NlfRtC3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now let us look at some of the technical details of the MANNCell. First, we discuss the main ingredients of the MANNCell, and initialization of the relevant units.\n",
        "*   The input arguments of the class initialization method `__init__` have already been specified, they will be used to initialize relevant structures in the class.\n",
        "*   `self.controller`: this is the controller of the MANN cell that is responsible for interfacing with the memory $M$. We recommend using `tf.keras.layers.LSTMCell` with `units=rnn_size` for initialization. For its technical details, see [tf.keras.layers.LSTMCell\n",
        "](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell).\n",
        "*   `self.controller_output_to_read_keys`, `self.controller_output_to_write_keys`, `self.controller_output_to_alphas`: the LSTM controller's output structure (we will discuss what its inputs should be later) is of the form [controller_output, controller_cell_and_hidden_states]. We need a mapping that maps the controller_output to the read keys, write keys and the interpolation coefficient $\\alpha_t$'s, which will then be used for interacting with the memory. Three `tf.keras.layers.Dense` layers (one for producing read keys, one for write keys, one for the $\\alpha_t$'s) are sufficient, though you are welcome to try out more complicated structures. \n",
        "  *  **Remark 1**: each access to memory involves `head_num` number of heads, if you wish, you could just initialize `self.controller_output_to_read_keys` with `units=self.memory_vector_dim*self.head_num` and apply `tf.split` to the output of the dense layer along `axis=1` and `num_or_size_splits=head_num` in the `call` method (similar for the other two dense layers).\n",
        "  *  ***From now on, we assume that you are following Remark 1 above in your implementation***.\n",
        "*    `self.controller_output_to_logits`: it should be a dense layer that will be used to map the concatenated controller_output + read_vector_list to the logits that will be used for obtaining the classification labels of the inputs and computing the cross entropy values. Thus, initialize it with `units=self.num_classes`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMM2bJpSR3EP",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "Finally, we discuss how to implement the method `call`. The following discussion is only one way of implementing the method, please feel free to deviate from it. However, ***we do suggest you to at least read through the discussion once***, as we have already implemented parts of the method and the whole training loop for you, and incompatibility between the data structures could cause the code to not run or have buggy outputs.\n",
        "*   **Caution**: even though most of the discussion below that involve tensors are treated either element-wise or vector-wise, in your implementation please utitlize tensorflow matrix operations as much as possible, as it can avoid strange bugs and increase the speed of your model.\n",
        "*   As described before, the input arguments of the `call` method are `inputs` and `states`. \n",
        "  *  Parse `state` to obtain `prev_controller_state`, `prev_read_vector_list`, `prev_w_r_list`, `prev_w_u`, `prev_M` that come from the previous iteration $t-1$. You may assume that they are zero-filled if $t=0$.\n",
        "*  Constructing the controller's input had been implemented for you. \n",
        "  *  The controller's output will be of the form `(controller_output, controller_states)`.\n",
        "  *  Why do you think  we should involve `prev_read_vector_list` in the controller's input?\n",
        "*  Now pass `controller_output` to the dense layers we discussed before, and obtain the read keys, write keys and the interpolation coefficients.\n",
        "  * Following the suggestion in the Remark 1 above, after applying `tf.split` to the dense layers' outputs, the shapes of your `read_key_list` and `write_key_list` should both be `(head_num, batch_size, memory_vector_dim)`, and the shape of `alpha_list` should be `(head_num, batch_size, 1)`. As an example, `read_key_list[i,p,:]` should be the memory read key for the $i$-th read head for the $p$-th sample in the batch.\n",
        "\n",
        "*  Before computing the read and write weights and interact with memory, we need to compute `prev_w_lu`, the least used weights from the previous iteration $t-1$. \n",
        "  *  You need to fill in the code for method `compute_w_lu`. To compute `prev_w_lu`, note that for the $p$-th sample in the batch in the previous iteration $t-1$, `prev_w_lu[p,:]` is a vector of binary values with length `num_memory_slots`: defining \n",
        "     \\begin{equation}\n",
        "      s(\\text{prev_w_u}[p,:], k)= \\text{the $k$-th smallest entry in prev_w_u}[p,:]\n",
        "     \\end{equation}\n",
        "     we have \n",
        "     \\begin{equation}\n",
        "      \\text{prev_w_lu}[p,i] = 0, \\;\\; \\text{if prev_w_u}[p,i] > s(\\text{prev_w_u}[p,:], \\text{head_num})\n",
        "     \\end{equation}\n",
        "     and \n",
        "     \\begin{equation}\n",
        "      \\text{prev_w_lu}[p,i]=1 \\;\\; \\text{otherwise}\n",
        "     \\end{equation}\n",
        "  *   Here is one way to implement `compute_w_lu`. Given input argument `prev_w_u` the usage weight from the previous iteration $t-1$ (it has shape `(batch_size, num_memory_slots)`), use `tf.math.top_k` to obtain the desired set of indices from `prev_w_u` (so you should have a `batch_size` number of index sets, each set is of size `head_num`; the overall structure should be of shape `(batch_size, head_num)`). Then use `tf.one_hot` and `tf.reduce_sum` to expand these indices into `prev_w_lu`, which should have shape `(batch_size, num_memory_slots)`. \n",
        "    *  From the set of indices with size `(batch_size, head_num)` you used for computing `prev_w_lu`, remember to also construct and return the index corresponding to *the smallest* entry in `prev_w_u[p,:]` for every $p$ (this index also correspond to the memory slot that was least used for the $p$-th sample in the previous iteration); so your returned indices will have size `(batch_size, 1)`.\n",
        "    *  You may find [tf.math.top_k\n",
        "](https://www.tensorflow.org/api_docs/python/tf/math/top_k), [tf.one_hot\n",
        "](https://www.tensorflow.org/api_docs/python/tf/one_hot) and [tf.reduce_sum\n",
        "](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum)  useful.\n",
        "    \n",
        "\n",
        "*  Now we proceed to compute the read and write weights $w^r_t$ and $w^w_t$.\n",
        "  *  For the $p$-th sample in the batch, recall that the read key `read_key_list[m,p,:]` is for the $m$-th read head for that sample, and `prev_M[p,j,:]` is the $j$-th memory vector for the $p$-th sample from the previous interation $t-1$ . Then the memory **read** weight `w_r_list[m,p,:]` for the $m$-th read head for the $p$-th sample is a 1D tensor with length `num_memory_slots`, with entries\n",
        "  \\begin{equation}\n",
        "    \\text{w_r_list}[m,p,i] = \\frac{\\exp(K(\\text{prev_M}[p,i,:],\\text{read_key_list}[m,p,:]))}{\\sum_{j=0}^{\\text{num_memory_slots}-1}\\exp(K(\\text{prev_M}[p,j,:], \\text{read_key_list}[m,p,:]))}\n",
        "  \\end{equation}\n",
        "  where $i=0,1,...,\\text{num_memory_slots}-1\\$, and\n",
        "  \\begin{equation}\n",
        "    K(x, y) = \\frac{x\\cdot y}{\\Vert x \\Vert_2 \\Vert y \\Vert_2 + \\epsilon}\n",
        "  \\end{equation}\n",
        "    *  $\\epsilon$ is there to ensure numerical stability. $\\epsilon=10^{-8}$ seems to be a good choice.\n",
        "    *  You might find some of the following tensorflow operations useful: [tf.matmul\n",
        "](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul), [tf.norm\n",
        "](https://www.tensorflow.org/api_docs/python/tf/norm), [tf.expand_dims\n",
        "](https://www.tensorflow.org/api_docs/python/tf/expand_dims), [tf.squeeze\n",
        "](https://www.tensorflow.org/api_docs/python/tf/squeeze), [tf.math.exp\n",
        "](https://www.tensorflow.org/api_docs/python/tf/math/exp) \n",
        "\n",
        "    *  In the suggested setup, the method `compute_read_weights`'s return shape should be `(batch_size, num_memory_slots)`, and `w_r_list` should have shape `(head_num, batch_size, num_memory_slots)`.\n",
        "\n",
        "  *  Given the $p$-th sample in the batch, the memory **write** weight `w_w_list[m,p,:]` for the $m$-th write head for that sample is of the general form:\n",
        "     \\begin{equation}\n",
        "      \\text{w_w_list}[m,p,i] = \\text{Sigmoid}(\\text{alpha_list}[m,p,0])\\times\\text{prev_w_r_list}[m,p,i] + (1 - \\text{Sigmoid}(\\text{alpha_list}[m,p,0]))\\times\\text{prev_w_lu}[p,i]\n",
        "     \\end{equation}\n",
        "     where $i=0,...,\\text{num_memory_slots-1}$.\n",
        "    *  In our suggested setup, method `compute_write_weights`'s return shape should be `(batch_size, num_memory_slots)`, so `w_w_list` should have shape `(head_num, batch_size, num_memory_slots)`.\n",
        "\n",
        "*  Let us read from memory `prev_M` now.\n",
        "    *  As we have `w_r_list` with shape `(head_num, batch_size, num_memory_slots)`, to obtain the read vectors, simply carry out the following: for the $m$-th read head for the $p$-th sample, \n",
        "      \\begin{equation}\n",
        "        \\text{read_vector_list}[m,p,:] = \\sum_{j=0}^{\\text{num_memory_slots}-1}\\text{w_r_list}[m,p,j]\\times\\text{prev_M}[p,j,:]\n",
        "      \\end{equation}\n",
        "      where `read_vector_list` has shape `(head_num, batch_size, memory_vector_dim)`.\n",
        "      *  Please remember that computing with matrices (in contrast to using some kind of for loop) can usually make you code run faster.\n",
        "\n",
        "* Having obtained the write weights `w_w_list`, we are closer to accessing the content of the memory now. But before that, rememeber that we got a set of indices of size `(batch_size, 1)` from the method `compute_w_lu` that indicated the least used memory slot in the previous iteration $t-1$? We are going to use them to zero out *the least used slot* in the memory first, before the writing operations.\n",
        "  *  One way of implementation: apply `tf.one_hot` to the set of indices of size `(batch_size, 1)` to obtain a matrix `E` of size `(batch_size, num_memory_slots)` containing one-hot vectors, where `E[p,j]` is 1 if the $j$-th memory slot for the $p$-th sample in the previous iteration was least used. Then we just need to compute the new memory along the line of $M*(1-E)$. So we have obtained `M_erased`, with shape `(batch_size, num_memory_slots, memory_vector_dim)`.\n",
        "\n",
        "* Now we can write to memory:\n",
        "  *  Recall that we have already computed `write_key_list` and `w_w_list` with shapes `(head_num, batch_size, memory_vector_dim)` and `(head_num, batch_size, num_memory_slots)` respectively. To write to `M_erased` with the $m$-th write head for the $p$-th sample, simply compute\n",
        "     \\begin{equation}\n",
        "      \\text{M_written}[p,i,:] = \\text{M_erased}[p,i,:] + \\text{w_w_list}[m,p,i]\\times\\text{write_key_list}[m,p,:]\n",
        "     \\end{equation}\n",
        "      *  You might find [tf.matmul\n",
        "](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) and [tf.expand_dims\n",
        "](https://www.tensorflow.org/api_docs/python/tf/expand_dims) useful here.\n",
        "  \n",
        "\n",
        "*  Finally, update the usage weight $w^u_t$ following the formula: for the $p$-th sample in the batch,\n",
        "   \\begin{equation}\n",
        "     \\text{w_u}[p,:] = \\text{self.gamma}\\times\\text{prev_w_u}[p,:] + \\sum_{i=0}^{\\text{head_num}-1}\\text{w_r_list}[i,p,:] + \\sum_{i=0}^{\\text{head_num}-1}\\text{w_w_list}[i,p,:]\n",
        "   \\end{equation}\n",
        "   where `w_u` has shape `(batch_size, num_memory_slots)`, and `self.gamma` is a manually defined free parameter of the model, which we have already set for you.\n",
        "*  Finally, we update the `state` dictionary , and feed [controller's output + the read vector list] to `self.controller_output_to_logits` which will be used for obtaining the labels for the input samples (already written for you) . Please ensure that all the relevant tensors have the correct shape and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwt3noNe_0PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MANNCell(tf.keras.layers.AbstractRNNCell):\n",
        "  def __init__(self, rnn_size, num_memory_slots, memory_vector_dim, head_num, num_classes=5, gamma=0.95, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    ################ Setup ###############################################\n",
        "    self.rnn_size = rnn_size\n",
        "    # number of memory slots\n",
        "    self.num_memory_slots = num_memory_slots\n",
        "    # size of each memory slot\n",
        "    self.memory_vector_dim = memory_vector_dim\n",
        "    self.head_num = head_num\n",
        "    # memory access head number is the same for both read and \n",
        "    # write in our setup  \n",
        "    self.write_head_num = head_num\n",
        "    # decay parameter for computing the usage weights\n",
        "    self.gamma = gamma\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    ########################################################################\n",
        "\n",
        "    # Controller RNN layer, we use an LSTM\n",
        "    # Recommended: tf.keras.layers.LSTMCell\n",
        "    \n",
        "    self.controller = tf.keras.layers.LSTMCell(self.rnn_size)\n",
        "    # controller_output \n",
        "    #          -> read_key (batch_size, head_num*memory_vector_dim)\n",
        "    #          -> write_key (batch_size, head_num*memory_vector_dim)\n",
        "    #          -> alpha (batch_size, head_num), interpolation coefficient for writing to memory\n",
        "    # We suggest units=self.memory_vector_dim*self.head_num for initializing the dense layers\n",
        "    # for read key and write keys, and units=self.head_num for the dense layer for alpha,\n",
        "    # and apply tf.split along axis=1 in the call method\n",
        "    self.controller_output_to_read_keys = tf.keras.layers.Dense(units=self.memory_vector_dim*self.head_num, use_bias=True)  \n",
        "    self.controller_output_to_write_keys = tf.keras.layers.Dense(units=self.memory_vector_dim*self.head_num,use_bias=True) \n",
        "    self.controller_output_to_alpha = tf.keras.layers.Dense(units=self.head_num,use_bias=True)                             \n",
        "\n",
        "    # This is the dense layer for mapping the controller output + read vector list to \n",
        "    # logits (which will then be used for computing the labels and cross-entropy values\n",
        "    # in NTMOneShotLearningModel). So initialize it with units=self.num_classes.\n",
        "    self.controller_output_to_logits = tf.keras.layers.Dense(units=self.num_classes) \n",
        "\n",
        "  @property\n",
        "  def state_size(self):\n",
        "    return self.rnn_size\n",
        "\n",
        "  # This initializes the dictionary states in MANNCell, and returns the initial state.\n",
        "  \n",
        "  def zero_state(self, batch_size, rnn_size, dtype):\n",
        "    one_hot_weight_vector = np.zeros([batch_size, self.num_memory_slots])\n",
        "    one_hot_weight_vector[..., 0] = 1\n",
        "    one_hot_weight_vector = tf.constant(one_hot_weight_vector, dtype=tf.float32)\n",
        "    initial_state = {\n",
        "            'controller_state': [tf.zeros((batch_size, rnn_size)), tf.zeros((batch_size, rnn_size))],\n",
        "            'read_vector_list': [tf.zeros([batch_size, self.memory_vector_dim])\n",
        "                                  for _ in range(self.head_num)],\n",
        "            'w_r_list': [one_hot_weight_vector for _ in range(self.head_num)],\n",
        "            'w_u': one_hot_weight_vector,\n",
        "            'M': tf.constant(np.ones([batch_size, self.num_memory_slots, self.memory_vector_dim]) * 1e-6, dtype=tf.float32)\n",
        "        }\n",
        "    return initial_state\n",
        "\n",
        "  def call(self, inputs, states):\n",
        "    # read vectors from the previous iteration, extract from states\n",
        "    prev_read_vector_list = states['read_vector_list'] \n",
        "    # state of controller from previous iteration t-1, extract from states\n",
        "    prev_controller_state = states['controller_state']  \n",
        "    # Obtain the list of w^r_{t-1}, M_{t-1}, and w^u_{t-1}, extract from states\n",
        "    prev_w_r_list = states['w_r_list'] \n",
        "    prev_M = states['M']\n",
        "    prev_w_u = states['w_u'] \n",
        "\n",
        "    # Controller output from the parameters of the read and write vectors\n",
        "    controller_input = tf.concat([inputs] + prev_read_vector_list, axis=1)\n",
        "    controller_output, controller_state = self.controller(inputs=controller_input, states=prev_controller_state)\n",
        "\n",
        "    # Map the controller_output to the read_keys, write_keys, and alphas\n",
        "    read_keys = self.controller_output_to_read_keys(controller_output)                      \n",
        "    write_keys = self.controller_output_to_write_keys(controller_output)  \n",
        "    alphas = self.controller_output_to_alpha(controller_output)            \n",
        "\n",
        "    # We have head_num heads per access to memory (same number of heads for read and write),\n",
        "    # so split the parameters obtained above into head_num groups, \n",
        "    # tf.split is useful here (try splitting along axis=1. Why?)\n",
        "    read_key_list = tf.tanh(tf.split(read_keys, axis=1, num_or_size_splits=self.head_num))\n",
        "    write_key_list = tf.tanh(tf.split(write_keys, axis=1, num_or_size_splits=self.head_num))\n",
        "    sig_alpha = tf.sigmoid(tf.split(alphas, axis=1, num_or_size_splits=self.head_num))\n",
        "\n",
        "    # For every p-th sample in the batch (from iteration t-1), compute the index \n",
        "    # corresponding to least used memory slot in prev_M[p,:,:], return as prev_indices.\n",
        "    # Also compute w^lu_{t-1}, return as prev_w_lu.\n",
        "     \n",
        "    prev_indices, prev_w_lu = self.compute_w_lu(prev_w_u)\n",
        "\n",
        "    # Setup read and write weights\n",
        "    w_r_list = []\n",
        "    w_w_list = []\n",
        "    # We obtain read and write weights for each head\n",
        "    for i in range(self.head_num):\n",
        "      # Obtain READ weights\n",
        "      \n",
        "      w_r = self.compute_read_weights(read_key_list[i], prev_M)\n",
        "      # Obtain WRITE weights\n",
        "      \n",
        "      w_w = self.compute_write_weights(sig_alpha[i], prev_w_r_list[i], prev_w_lu)\n",
        "      # Note: w_r_list is of shape (head_num, batch_size, num_memory_slots), \n",
        "      # and same for w_w_list\n",
        "      w_r_list.append(w_r)\n",
        "      w_w_list.append(w_w)\n",
        "\n",
        "    # Set least used memory slot in prev_M to ZERO, make use of prev_indices!\n",
        "    M_erased = prev_M * tf.expand_dims(1. - tf.one_hot(prev_indices[:, -1], self.num_memory_slots), dim=2) \n",
        "    M_written = M_erased\n",
        "\n",
        "    # Read from memory M_{t-1}, using the w_r_list\n",
        "    read_vector_list = []\n",
        "    # Iterate over each head\n",
        "    for i in range(self.head_num):\n",
        "      \n",
        "      read_vector = tf.reduce_sum(tf.expand_dims(w_r_list[i], dim=2) * M_written, axis=1)\n",
        "      # read_vector_list should have shape (head_num, batch_size, memory_vector_dim)\n",
        "      read_vector_list.append(read_vector)\n",
        "\n",
        "    # Write to memory, form M_t, using the w_w_list and write_keys\n",
        "    # Iterate over each head\n",
        "    for i in range(self.head_num):\n",
        "      \n",
        "      w = tf.expand_dims(w_w_list[i], axis=2)\n",
        "      k = tf.expand_dims(write_key_list[i], axis=1)\n",
        "      M_written = M_written + tf.matmul(w,k) \n",
        "\n",
        "    # Compute usage weights w^u_t for the current iteration\n",
        "    w_u = self.gamma * prev_w_u + tf.add_n(w_r_list) + tf.add_n(w_w_list)   # eq (20) \n",
        "\n",
        "    # Concatenate controller's output and the read memory\n",
        "    # content, they are then fed into a dense layer to obtain the logits,\n",
        "    # which will be used for obtaininig labels and computing the  cross-entropy \n",
        "    # values in NTMOneShotLearningModel below\n",
        "    mann_output = tf.concat([controller_output] + read_vector_list, axis=1)\n",
        "    logits =self.controller_output_to_logits(mann_output) \n",
        "\n",
        "    state = {\n",
        "        'controller_state': controller_state,\n",
        "        'read_vector_list': read_vector_list,\n",
        "        'w_r_list': w_r_list,\n",
        "        'w_w_list': w_w_list,\n",
        "        'w_u': w_u,\n",
        "        'M': M_written,\n",
        "    }\n",
        "\n",
        "    return logits, state\n",
        "\n",
        "  def compute_read_weights(self, read_key, prev_M):\n",
        "    \n",
        "    # Compute the inner products, norms\n",
        "    # Compute the exp(K(M,key))'s\n",
        "    # Obtain read weights\n",
        "\n",
        "    with tf.variable_scope('compute_read_weights'):\n",
        "      # Cosine Similarity\n",
        "\n",
        "      read_key = tf.expand_dims(read_key, axis=2)\n",
        "      inner_product = tf.matmul(prev_M, read_key)\n",
        "      read_key_norm = tf.sqrt(tf.reduce_sum(tf.square(read_key), axis=1, keep_dims=True))\n",
        "      M_norm = tf.sqrt(tf.reduce_sum(tf.square(prev_M), axis=2, keep_dims=True))\n",
        "      norm_product = M_norm * read_key_norm\n",
        "      read_K = tf.squeeze(inner_product / (norm_product + 1e-8))                   # eq (17)\n",
        "\n",
        "      # Calculating w^c\n",
        "\n",
        "      K_exp = tf.exp(read_K)\n",
        "      w_r = K_exp / tf.reduce_sum(K_exp, axis=1, keep_dims=True)                # eq (18)\n",
        "\n",
        "      return w_r\n",
        "\n",
        "  def compute_write_weights(self, sig_alpha, prev_w_r, prev_w_lu):\n",
        "    # Compute the write weights\n",
        "    \n",
        "    with tf.variable_scope('compute_write_weights'):\n",
        "\n",
        "      # Write to (1) the place that was read in t-1 (2) the place that was least used in t-1\n",
        "      w_w = sig_alpha * prev_w_r + (1. - sig_alpha) * prev_w_lu\n",
        "      \n",
        "      return w_w\n",
        "\n",
        "  def compute_w_lu(self, prev_w_u):\n",
        "    ]\n",
        "    _, indices = tf.nn.top_k(prev_w_u, k=self.num_memory_slots)\n",
        "    prev_w_lu = tf.reduce_sum(tf.one_hot(indices[:, -self.head_num:], depth=self.num_memory_slots), axis=1)\n",
        "    return indices, prev_w_lu\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZTXPodW_5_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NTMOneShotLearningModel():\n",
        "  def __init__(self, model, n_classes, batch_size, seq_length, image_width, image_height,\n",
        "                rnn_size, num_memory_slots, rnn_num_layers, read_head_num, write_head_num, memory_vector_dim, learning_rate):\n",
        "    self.output_dim = n_classes\n",
        "\n",
        "    # Note: the images are flattened to 1D tensors\n",
        "    # The input data structure is of the following form:\n",
        "    # self.x_image[i,j,:] = jth image in the ith sequence (or, episode)\n",
        "    self.x_image = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, image_width * image_height])\n",
        "    # Model's output label is one-hot encoded\n",
        "    # The data structure is of the following form:\n",
        "    # self.x_label[i,j,:] = one-hot label of the jth image in \n",
        "    #             the ith sequence (or, episode)\n",
        "    self.x_label = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n",
        "    # Target label is one-hot encoded\n",
        "    self.y = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n",
        "\n",
        "    if model == 'LSTM':\n",
        "      # Using a LSTM layer to serve as the controller, no memory\n",
        "      def rnn_cell(rnn_size):\n",
        "        return tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
        "      cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(rnn_size) for _ in range(rnn_num_layers)])\n",
        "      state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
        "    elif model == 'MANN':\n",
        "      # Using a MANN network as the controller, with memory\n",
        "      cell = MANNCell(rnn_size, num_memory_slots, memory_vector_dim,\n",
        "                                head_num=read_head_num)\n",
        "      state = cell.zero_state(batch_size=batch_size, rnn_size=rnn_size, dtype=tf.float32)\n",
        "    \n",
        "    \n",
        "    self.state_list = [state]\n",
        "    # Setup the NTM's output\n",
        "    self.o = []\n",
        "    \n",
        "    # Now iterate over every sample in the sequence \n",
        "    for t in range(seq_length):\n",
        "      output, state = cell(tf.concat([self.x_image[:, t, :], self.x_label[:, t, :]], axis=1), state)\n",
        "      output = tf.nn.softmax(output, axis=1)\n",
        "      self.o.append(output)\n",
        "      self.state_list.append(state)\n",
        "    # post-process the output of the classifier\n",
        "    self.o = tf.stack(self.o, axis=1)\n",
        "    self.state_list.append(state)\n",
        "\n",
        "    eps = 1e-8\n",
        "    # cross entropy, between model output labels and target labels\n",
        "    self.learning_loss = -tf.reduce_mean(  \n",
        "        tf.reduce_sum(self.y * tf.log(self.o + eps), axis=[1, 2])\n",
        "    )\n",
        "    \n",
        "    self.o = tf.reshape(self.o, shape=[batch_size, seq_length, -1])\n",
        "    self.learning_loss_summary = tf.summary.scalar('learning_loss', self.learning_loss)\n",
        "\n",
        "    with tf.variable_scope('optimizer'):\n",
        "      self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "      self.train_op = self.optimizer.minimize(self.learning_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_qMlbTWAvg0",
        "colab_type": "text"
      },
      "source": [
        "The training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se1yEaxmey6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n",
        "         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir):\n",
        "  \n",
        "  # We always use one-hot encoding of the labels in this experiment\n",
        "  label_type = \"one_hot\"\n",
        "\n",
        "  # Initialize the model\n",
        "  model = NTMOneShotLearningModel(model=model_path, n_classes=n_classes,\\\n",
        "                    batch_size=batch_size, seq_length=seq_length,\\\n",
        "                    image_width=image_width, image_height=image_height, \\\n",
        "                    rnn_size=rnn_size, num_memory_slots=num_memory_slots,\\\n",
        "                    rnn_num_layers=rnn_num_layers, read_head_num=read_head_num,\\\n",
        "                    write_head_num=write_head_num, memory_vector_dim=memory_vector_dim,\\\n",
        "                    learning_rate=learning_rate)\n",
        "  print(\"Model initialized\")\n",
        "  data_loader = OmniglotDataLoader(\n",
        "      image_size=(image_width, image_height),\n",
        "      n_train_classses=n_train_classes,\n",
        "      n_test_classes=n_test_classes\n",
        "  )\n",
        "  print(\"Data loaded\")\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    if restore_training:\n",
        "      saver = tf.train.Saver()\n",
        "      ckpt = tf.train.get_checkpoint_state(save_dir + '/' + model_path)\n",
        "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    else:\n",
        "      saver = tf.train.Saver(tf.global_variables())\n",
        "      tf.global_variables_initializer().run()\n",
        "    train_writer = tf.summary.FileWriter(tensorboard_dir + '/' + model_path, sess.graph)\n",
        "    print(\"1st\\t2nd\\t3rd\\t4th\\t5th\\t6th\\t7th\\t8th\\t9th\\t10th\\tepoch\\tloss\")\n",
        "    for b in range(num_epochs):\n",
        "      # Test the model\n",
        "      if b % 100 == 0:\n",
        "        # Note: the images are flattened to 1D tensors\n",
        "        # The input data structure is of the following form:\n",
        "        # x_image[i,j,:] = jth image in the ith sequence (or, episode)\n",
        "        # And the sequence of 50 images x_image[i,:,:] constitute\n",
        "        # one episode, and each class (out of 5 classes) has around 10\n",
        "        # appearances in this sequence, as seq_length = 50 and \n",
        "        # n_classes = 5, as specified in the code block below\n",
        "        # See the details in utils.py, OmniglotDataLoader class\n",
        "        x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length,\n",
        "                                  type='test',\n",
        "                                  augment=augment,\n",
        "                                  label_type=label_type)\n",
        "        feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n",
        "        output, learning_loss = sess.run([model.o, model.learning_loss], feed_dict=feed_dict)\n",
        "        merged_summary = sess.run(model.learning_loss_summary, feed_dict=feed_dict)\n",
        "        train_writer.add_summary(merged_summary, b)\n",
        "        accuracy = test(seq_length, y, output)\n",
        "        for accu in accuracy:\n",
        "          print('%.4f' % accu, end='\\t')\n",
        "        print('%d\\t%.4f' % (b, learning_loss))\n",
        "\n",
        "      # Save model per 2000 epochs\n",
        "      if b%2000==0 and b>0:\n",
        "        saver.save(sess, save_dir + '/' + model_path + '/model.tfmodel', global_step=b)\n",
        "\n",
        "      # Train the model\n",
        "      x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length, \\\n",
        "                                type='train',\n",
        "                                augment=augment,\n",
        "                                label_type=label_type)\n",
        "      feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n",
        "      sess.run(model.train_op, feed_dict=feed_dict)\n",
        "\n",
        "\n",
        "def test(seq_length, y, output):\n",
        "\n",
        "  total_acc = np.zeros(10)\n",
        "  total_class = np.zeros(10)\n",
        "  for l in range(output.shape[0]):\n",
        "    class_tracker = np.zeros(5)\n",
        "    acc = np.zeros(50)\n",
        "    for i in range(seq_length):\n",
        "      class_tracker[np.argmax(output[l][i])] = class_tracker[np.argmax(output[l][i])] + 1\n",
        "      j = int(round(class_tracker[np.argmax(output[l][i])]))\n",
        "      if np.argmax(output[l][i]) == np.argmax(y[l][i]):\n",
        "        acc[j] = acc[j] + 1\n",
        "    for j in range(10):\n",
        "      total_acc[j] = total_acc[j] + acc[j]\n",
        "      total_class[j] = total_class[j] + len(class_tracker[class_tracker >= j])\n",
        "  return ((total_acc/total_class))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VruOInLHkZUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2724
        },
        "outputId": "ea7bc96f-678a-410d-f97b-966c657bdfa9"
      },
      "source": [
        "  restore_training = False\n",
        "label_type = \"one_hot\"\n",
        "n_classes = 5\n",
        "seq_length = 50\n",
        "augment = True\n",
        "read_head_num = 4\n",
        "batch_size = 16\n",
        "num_epochs = 100000\n",
        "learning_rate = 1e-3\n",
        "rnn_size = 200\n",
        "image_width = 20\n",
        "image_height = 20\n",
        "rnn_num_layers = 1\n",
        "num_memory_slots = 128\n",
        "memory_vector_dim = 40\n",
        "shift_range = 1\n",
        "write_head_num = 4\n",
        "test_batch_num = 100\n",
        "n_train_classes = 220\n",
        "n_test_classes = 60\n",
        "save_dir = './save/one_shot_learning'\n",
        "tensorboard_dir = './summary/one_shot_learning'\n",
        "model_path = 'MANN'\n",
        "train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n",
        "         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-92f2603fcf2e>:161: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the `axis` argument instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model initialized\n",
            "Entered Dataloader\n",
            "10.0% data loaded.\n",
            "20.0% data loaded.\n",
            "30.0% data loaded.\n",
            "40.0% data loaded.\n",
            "50.0% data loaded.\n",
            "60.0% data loaded.\n",
            "70.0% data loaded.\n",
            "80.0% data loaded.\n",
            "90.0% data loaded.\n",
            "100.0% data loaded.\n",
            "Data loaded\n",
            "1st\t2nd\t3rd\t4th\t5th\t6th\t7th\t8th\t9th\t10th\tepoch\tloss\n",
            "0.0000\t0.1897\t0.2321\t0.2115\t0.2692\t0.2000\t0.2609\t0.2093\t0.2927\t0.1081\t0\t80.7760\n",
            "0.0000\t0.1579\t0.1370\t0.1127\t0.1343\t0.1475\t0.1930\t0.3091\t0.1633\t0.2051\t100\t80.6095\n",
            "0.0000\t0.1739\t0.1587\t0.2586\t0.2745\t0.2128\t0.1860\t0.2381\t0.1622\t0.1471\t200\t80.7303\n",
            "0.0000\t0.2025\t0.2564\t0.2059\t0.1613\t0.2407\t0.1875\t0.1556\t0.2093\t0.0769\t300\t80.4539\n",
            "0.0000\t0.1892\t0.2388\t0.2679\t0.1163\t0.2286\t0.2333\t0.2609\t0.2857\t0.2000\t400\t80.3608\n",
            "0.0000\t0.1867\t0.1884\t0.1935\t0.1636\t0.2000\t0.3182\t0.1282\t0.1515\t0.1429\t500\t80.4559\n",
            "0.0000\t0.1375\t0.1772\t0.1711\t0.1892\t0.1739\t0.2419\t0.1636\t0.1064\t0.2326\t600\t80.5447\n",
            "0.0000\t0.1711\t0.1143\t0.1304\t0.2500\t0.1967\t0.2414\t0.2037\t0.2708\t0.1915\t700\t80.5369\n",
            "0.0000\t0.3167\t0.1786\t0.2075\t0.1400\t0.2174\t0.1395\t0.1579\t0.2632\t0.1389\t800\t80.4833\n",
            "0.0000\t0.0968\t0.1765\t0.1778\t0.2195\t0.2432\t0.1765\t0.2188\t0.1034\t0.1724\t900\t80.5052\n",
            "0.0000\t0.1625\t0.2500\t0.1667\t0.1692\t0.1607\t0.2174\t0.1538\t0.1667\t0.2000\t1000\t80.5193\n",
            "0.0000\t0.2432\t0.2969\t0.2353\t0.2222\t0.3000\t0.1579\t0.1714\t0.1212\t0.1515\t1100\t80.5377\n",
            "0.0000\t0.2424\t0.1803\t0.1695\t0.2593\t0.3269\t0.1915\t0.1707\t0.1081\t0.3333\t1200\t80.4662\n",
            "0.0000\t0.2400\t0.2286\t0.1774\t0.2321\t0.1600\t0.2326\t0.2286\t0.3226\t0.2857\t1300\t80.4789\n",
            "0.0000\t0.2388\t0.1967\t0.1429\t0.3091\t0.2857\t0.2174\t0.2045\t0.2632\t0.1143\t1400\t80.5234\n",
            "0.0000\t0.1500\t0.2027\t0.1493\t0.1311\t0.2281\t0.2075\t0.2083\t0.1591\t0.1250\t1500\t80.5002\n",
            "0.0000\t0.2078\t0.1912\t0.2769\t0.3103\t0.2115\t0.2000\t0.1064\t0.2000\t0.3182\t1600\t80.4387\n",
            "0.0000\t0.2090\t0.2333\t0.1852\t0.2041\t0.0667\t0.2143\t0.3077\t0.1389\t0.2188\t1700\t80.4611\n",
            "0.0000\t0.1791\t0.1481\t0.2549\t0.1837\t0.1304\t0.1905\t0.1750\t0.1579\t0.2368\t1800\t80.4361\n",
            "0.0000\t0.1250\t0.1143\t0.2424\t0.2188\t0.1562\t0.1562\t0.4375\t0.0938\t0.1875\t1900\t80.4614\n",
            "0.0000\t0.1739\t0.1803\t0.3333\t0.2653\t0.2273\t0.1667\t0.1951\t0.1875\t0.1071\t2000\t80.4453\n",
            "0.0000\t0.2619\t0.0741\t0.2000\t0.0000\t0.2500\t0.1250\t0.1250\t0.1250\t0.2500\t2100\t80.4677\n",
            "0.0000\t0.2152\t0.1667\t0.2059\t0.2063\t0.1667\t0.2979\t0.1395\t0.1190\t0.1892\t2200\t80.4882\n",
            "0.0000\t0.2055\t0.2273\t0.1875\t0.2419\t0.1167\t0.2373\t0.2241\t0.1765\t0.1556\t2300\t80.4883\n",
            "0.0000\t0.3103\t0.3125\t0.2195\t0.2368\t0.2222\t0.1714\t0.1714\t0.2353\t0.1379\t2400\t80.4740\n",
            "0.0000\t0.2742\t0.1667\t0.1765\t0.1087\t0.1860\t0.1707\t0.2564\t0.1579\t0.2105\t2500\t80.4832\n",
            "0.0000\t0.2903\t0.1786\t0.1042\t0.2619\t0.3514\t0.1290\t0.1379\t0.2000\t0.2174\t2600\t80.4833\n",
            "0.0000\t0.1967\t0.1400\t0.1250\t0.2444\t0.1429\t0.1316\t0.2162\t0.2188\t0.2258\t2700\t80.4696\n",
            "0.0000\t0.2121\t0.3276\t0.1905\t0.2059\t0.2069\t0.2593\t0.1923\t0.2800\t0.1818\t2800\t80.4602\n",
            "0.0000\t0.1875\t0.2500\t0.1389\t0.1765\t0.3235\t0.2059\t0.3030\t0.2121\t0.2188\t2900\t80.4702\n",
            "0.0000\t0.2419\t0.1667\t0.3269\t0.1633\t0.1633\t0.1702\t0.2683\t0.1795\t0.2368\t3000\t80.4433\n",
            "0.0000\t0.2419\t0.2500\t0.2500\t0.2632\t0.1579\t0.2105\t0.2778\t0.2353\t0.1515\t3100\t80.4494\n",
            "0.0000\t0.1765\t0.2195\t0.1892\t0.3793\t0.3478\t0.1429\t0.2000\t0.3158\t0.1053\t3200\t80.4200\n",
            "0.0000\t0.2609\t0.2093\t0.2308\t0.2432\t0.2353\t0.2414\t0.1786\t0.2083\t0.0833\t3300\t80.4715\n",
            "0.0000\t0.2549\t0.2500\t0.2083\t0.3542\t0.1667\t0.2340\t0.2609\t0.2045\t0.1395\t3400\t80.4264\n",
            "0.0000\t0.2361\t0.2090\t0.1930\t0.2037\t0.2041\t0.1875\t0.2791\t0.3889\t0.1212\t3500\t79.5842\n",
            "0.0000\t0.2911\t0.2838\t0.2676\t0.2985\t0.3167\t0.2881\t0.3774\t0.4600\t0.4255\t3600\t71.3059\n",
            "0.0000\t0.3590\t0.3731\t0.5312\t0.5345\t0.6182\t0.5400\t0.5714\t0.5833\t0.5116\t3700\t57.6024\n",
            "0.0000\t0.2658\t0.4730\t0.4861\t0.5217\t0.6000\t0.5484\t0.5932\t0.5577\t0.5208\t3800\t53.2393\n",
            "0.0000\t0.4304\t0.4810\t0.5789\t0.5867\t0.5571\t0.6190\t0.5862\t0.5536\t0.5957\t3900\t47.5256\n",
            "0.0000\t0.4875\t0.5443\t0.6104\t0.5600\t0.6389\t0.7273\t0.5263\t0.5294\t0.5682\t4000\t49.1769\n",
            "0.0000\t0.2500\t0.4805\t0.7067\t0.6575\t0.7164\t0.7460\t0.6393\t0.5273\t0.7451\t4100\t43.2778\n",
            "0.0000\t0.3500\t0.5256\t0.6104\t0.6027\t0.6522\t0.6866\t0.6667\t0.6667\t0.7447\t4200\t49.3955\n",
            "0.0000\t0.2625\t0.6364\t0.5541\t0.7222\t0.6957\t0.6471\t0.6393\t0.6393\t0.7170\t4300\t45.2324\n",
            "0.0000\t0.4500\t0.5500\t0.6500\t0.6709\t0.7432\t0.6575\t0.6667\t0.7241\t0.7593\t4400\t43.0226\n",
            "0.0000\t0.4375\t0.5625\t0.6962\t0.5395\t0.6800\t0.7297\t0.6286\t0.6923\t0.6182\t4500\t44.1040\n",
            "0.0000\t0.3924\t0.6329\t0.7089\t0.6400\t0.7746\t0.7385\t0.6333\t0.7455\t0.7551\t4600\t38.8314\n",
            "0.0000\t0.4375\t0.5625\t0.7215\t0.6364\t0.7671\t0.7746\t0.6875\t0.8361\t0.7818\t4700\t36.9349\n",
            "0.0000\t0.3875\t0.5500\t0.6456\t0.7595\t0.6538\t0.6974\t0.7101\t0.8571\t0.7037\t4800\t38.3108\n",
            "0.0000\t0.3625\t0.5500\t0.7375\t0.7179\t0.8158\t0.7973\t0.7353\t0.7869\t0.7200\t4900\t36.2897\n",
            "0.0000\t0.4177\t0.5065\t0.6974\t0.7432\t0.7260\t0.7206\t0.7385\t0.9138\t0.7600\t5000\t37.7376\n",
            "0.0000\t0.4000\t0.5750\t0.6923\t0.7143\t0.7733\t0.8767\t0.7941\t0.8621\t0.9231\t5100\t32.9752\n",
            "0.0000\t0.4750\t0.6203\t0.6076\t0.6494\t0.7361\t0.6866\t0.8154\t0.6935\t0.7321\t5200\t39.0130\n",
            "0.0000\t0.4500\t0.6000\t0.7215\t0.6447\t0.8289\t0.7808\t0.7101\t0.8103\t0.8519\t5300\t31.3940\n",
            "0.0000\t0.4625\t0.5375\t0.6500\t0.7051\t0.7821\t0.7922\t0.8056\t0.7910\t0.8333\t5400\t34.1349\n",
            "0.0000\t0.4375\t0.5125\t0.6709\t0.6753\t0.7895\t0.7123\t0.6957\t0.7213\t0.7115\t5500\t36.2403\n",
            "0.0000\t0.3924\t0.5696\t0.7532\t0.7532\t0.7368\t0.6901\t0.7761\t0.8125\t0.8727\t5600\t34.0349\n",
            "0.0000\t0.4750\t0.6750\t0.7375\t0.8228\t0.7703\t0.8219\t0.7887\t0.8182\t0.8846\t5700\t29.4932\n",
            "0.0000\t0.4500\t0.6203\t0.6835\t0.6962\t0.7260\t0.7971\t0.7727\t0.7812\t0.7679\t5800\t32.7849\n",
            "0.0000\t0.4875\t0.6000\t0.8500\t0.8228\t0.8378\t0.8551\t0.7910\t0.8033\t0.9630\t5900\t29.2071\n",
            "0.0000\t0.4000\t0.6625\t0.7692\t0.7368\t0.7568\t0.8261\t0.7656\t0.8500\t0.9057\t6000\t31.3750\n",
            "0.0000\t0.4500\t0.5823\t0.7792\t0.8000\t0.8243\t0.7746\t0.8310\t0.8730\t0.8214\t6100\t30.6230\n",
            "0.0000\t0.3974\t0.5641\t0.7949\t0.7179\t0.7143\t0.8667\t0.7571\t0.7460\t0.7778\t6200\t33.9954\n",
            "0.0000\t0.5500\t0.5949\t0.7342\t0.7848\t0.7922\t0.8630\t0.8382\t0.8500\t0.7547\t6300\t30.3123\n",
            "0.0000\t0.4000\t0.6000\t0.6500\t0.7949\t0.7500\t0.8333\t0.8333\t0.8644\t0.8163\t6400\t30.1115\n",
            "0.0000\t0.4500\t0.6625\t0.8000\t0.7848\t0.7821\t0.7838\t0.7826\t0.9138\t0.7600\t6500\t28.7830\n",
            "0.0000\t0.4625\t0.7625\t0.7051\t0.7821\t0.8052\t0.8684\t0.8116\t0.7619\t0.8824\t6600\t28.8062\n",
            "0.0000\t0.4250\t0.6125\t0.6875\t0.8101\t0.7821\t0.8421\t0.8310\t0.8182\t0.7692\t6700\t31.2252\n",
            "0.0000\t0.4500\t0.6750\t0.7089\t0.8701\t0.8158\t0.8421\t0.9054\t0.8788\t0.8727\t6800\t25.8909\n",
            "0.0000\t0.5000\t0.6456\t0.8228\t0.7564\t0.7162\t0.8732\t0.8507\t0.8167\t0.8182\t6900\t28.5651\n",
            "0.0000\t0.5125\t0.6250\t0.6375\t0.8125\t0.7867\t0.8611\t0.8116\t0.8462\t0.8750\t7000\t28.5468\n",
            "0.0000\t0.4625\t0.6125\t0.8250\t0.8462\t0.8312\t0.8400\t0.8696\t0.9048\t0.8491\t7100\t25.7788\n",
            "0.0000\t0.4875\t0.6375\t0.7500\t0.8228\t0.7722\t0.8919\t0.8056\t0.8923\t0.8596\t7200\t27.7458\n",
            "0.0000\t0.4625\t0.6329\t0.7692\t0.8052\t0.8026\t0.8732\t0.9206\t0.8814\t0.9216\t7300\t24.4445\n",
            "0.0000\t0.3875\t0.6375\t0.7625\t0.8205\t0.7467\t0.8243\t0.9077\t0.9508\t0.8462\t7400\t27.9555\n",
            "0.0000\t0.4500\t0.7089\t0.7848\t0.8182\t0.8800\t0.8333\t0.7761\t0.8833\t0.8298\t7500\t26.7635\n",
            "0.0000\t0.4750\t0.6625\t0.7375\t0.8354\t0.7342\t0.9079\t0.8732\t0.9275\t0.9661\t7600\t25.4919\n",
            "0.0000\t0.5063\t0.6329\t0.8354\t0.7848\t0.8312\t0.9054\t0.8056\t0.8769\t0.8947\t7700\t26.2856\n",
            "0.0000\t0.4875\t0.6750\t0.7975\t0.8101\t0.8312\t0.9200\t0.8904\t0.9697\t0.9259\t7800\t22.8352\n",
            "0.0000\t0.4500\t0.6582\t0.7595\t0.8987\t0.9474\t0.8267\t0.9275\t0.9516\t0.8600\t7900\t22.0531\n",
            "0.0000\t0.4625\t0.6250\t0.7625\t0.8625\t0.8101\t0.8442\t0.7714\t0.8594\t0.8846\t8000\t27.7452\n",
            "0.0000\t0.4625\t0.5949\t0.7595\t0.7949\t0.9091\t0.8000\t0.9429\t0.8548\t0.9623\t8100\t23.9822\n",
            "0.0000\t0.5375\t0.7000\t0.7468\t0.8718\t0.8442\t0.9200\t0.9189\t0.8923\t0.8644\t8200\t21.1960\n",
            "0.0000\t0.4500\t0.6250\t0.7750\t0.8861\t0.8553\t0.8611\t0.9420\t0.8594\t0.8596\t8300\t24.0524\n",
            "0.0000\t0.4750\t0.7342\t0.7821\t0.8816\t0.8553\t0.8767\t0.8000\t0.9538\t0.9333\t8400\t22.8560\n",
            "0.0000\t0.4250\t0.7595\t0.8354\t0.8734\t0.9091\t0.8816\t0.9333\t0.9394\t0.9464\t8500\t19.5597\n",
            "0.0000\t0.5250\t0.7089\t0.8734\t0.8734\t0.8846\t0.9155\t0.8971\t0.8730\t0.8889\t8600\t23.8648\n",
            "0.0000\t0.4625\t0.7250\t0.8000\t0.8625\t0.8861\t0.8816\t0.8429\t0.9310\t0.8868\t8700\t21.4465\n",
            "0.0000\t0.5500\t0.5949\t0.7975\t0.9241\t0.8846\t0.9595\t0.8873\t0.9231\t0.8868\t8800\t19.8386\n",
            "0.0000\t0.4500\t0.7250\t0.8625\t0.8734\t0.9114\t0.8816\t0.8630\t0.8689\t0.8800\t8900\t21.8576\n",
            "0.0000\t0.4875\t0.6875\t0.7125\t0.8250\t0.8312\t0.9067\t0.8592\t0.9231\t0.9310\t9000\t23.4630\n",
            "0.0000\t0.3375\t0.6250\t0.7875\t0.8625\t0.9103\t0.8933\t0.8750\t0.9365\t0.8889\t9100\t22.9755\n",
            "0.0000\t0.4375\t0.6750\t0.8875\t0.8228\t0.8734\t0.8800\t0.9143\t0.8689\t0.7963\t9200\t22.2671\n",
            "0.0000\t0.4125\t0.7375\t0.7750\t0.8861\t0.9367\t0.9351\t0.8592\t0.8689\t0.8654\t9300\t21.7038\n",
            "0.0000\t0.3750\t0.6375\t0.8375\t0.8608\t0.9079\t0.8816\t0.9143\t0.9032\t0.9286\t9400\t20.4694\n",
            "0.0000\t0.5125\t0.6875\t0.7250\t0.8625\t0.8684\t0.8630\t0.9143\t0.9138\t0.8600\t9500\t22.1477\n",
            "0.0000\t0.5000\t0.7125\t0.9000\t0.8987\t0.9103\t0.9577\t0.9706\t0.9385\t0.9091\t9600\t17.4001\n",
            "0.0000\t0.5250\t0.6875\t0.8625\t0.8861\t0.8734\t0.8933\t0.8611\t0.9091\t0.9310\t9700\t22.0225\n",
            "0.0000\t0.4750\t0.7089\t0.8101\t0.8608\t0.9359\t0.9221\t0.8873\t0.8788\t0.9020\t9800\t19.6250\n",
            "0.0000\t0.5250\t0.6875\t0.8375\t0.8625\t0.8846\t0.9315\t0.9559\t0.8833\t0.9167\t9900\t19.4325\n",
            "0.0000\t0.5500\t0.7250\t0.7375\t0.7875\t0.8947\t0.8493\t0.9412\t0.9516\t0.8947\t10000\t21.2882\n",
            "0.0000\t0.4500\t0.6125\t0.8250\t0.8590\t0.9079\t0.9067\t0.9306\t0.8806\t0.8889\t10100\t20.5965\n",
            "0.0000\t0.5625\t0.7375\t0.8250\t0.8734\t0.9342\t0.9178\t0.9412\t0.9180\t0.9057\t10200\t19.0586\n",
            "0.0000\t0.4875\t0.6875\t0.8500\t0.9125\t0.8608\t0.9605\t0.9155\t0.9365\t0.8983\t10300\t18.4338\n",
            "0.0000\t0.4000\t0.7250\t0.7848\t0.9103\t0.8846\t0.9605\t0.9091\t0.9206\t0.9636\t10400\t19.0579\n",
            "0.0000\t0.5250\t0.7000\t0.8481\t0.9241\t0.8846\t0.9067\t0.9296\t0.9403\t0.9661\t10500\t18.6116\n",
            "0.0000\t0.5625\t0.6250\t0.7975\t0.8734\t0.9114\t0.9067\t0.9848\t0.9180\t0.9643\t10600\t17.6186\n",
            "0.0000\t0.5375\t0.7625\t0.8481\t0.8987\t0.9359\t0.9221\t0.9444\t0.9365\t0.9811\t10700\t17.0758\n",
            "0.0000\t0.3750\t0.7375\t0.8375\t0.9241\t0.9103\t0.9726\t0.9143\t0.9500\t0.8824\t10800\t18.4788\n",
            "0.0000\t0.5375\t0.7500\t0.8125\t0.8750\t0.9125\t0.9359\t0.9324\t0.9545\t0.9455\t10900\t17.0346\n",
            "0.0000\t0.4125\t0.6625\t0.8625\t0.9103\t0.8933\t0.9296\t1.0000\t0.9552\t0.9492\t11000\t16.8671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e1c54df2fba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtensorboard_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./summary/one_shot_learning'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MANN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_training\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_memory_slots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-9d99edaa0363>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir)\u001b[0m\n\u001b[1;32m     55\u001b[0m       x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length,                                 type='train',\n\u001b[1;32m     56\u001b[0m                                 \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                                 label_type=label_type)\n\u001b[0m\u001b[1;32m     58\u001b[0m       \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_image\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/NTM_Student/utils.py\u001b[0m in \u001b[0;36mfetch_batch\u001b[0;34m(self, n_classes, batch_size, seq_length, type, sample_strategy, augment, label_type)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                  only_resize=not augment)\n\u001b[1;32m     80\u001b[0m                    for j in seq[i, :]]\n\u001b[0;32m---> 81\u001b[0;31m                    for i in range(batch_size)]\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'one_hot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mseq_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/NTM_Student/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                  only_resize=not augment)\n\u001b[1;32m     80\u001b[0m                    for j in seq[i, :]]\n\u001b[0;32m---> 81\u001b[0;31m                    for i in range(batch_size)]\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'one_hot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mseq_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/NTM_Student/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m                                  \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                                  only_resize=not augment)\n\u001b[0;32m---> 80\u001b[0;31m                    for j in seq[i, :]]\n\u001b[0m\u001b[1;32m     81\u001b[0m                    for i in range(batch_size)]\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'one_hot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/NTM_Student/utils.py\u001b[0m in \u001b[0;36maugment\u001b[0;34m(self, image, batch, c, only_resize)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mrand_rotate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_rotate_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m90\u001b[0m                       \u001b[0;31m# rotate by 0, pi/2, pi, 3pi/2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 .rotate(rand_rotate + np.random.rand() * 22.5 - 11.25,\n\u001b[1;32m    108\u001b[0m                         translate=np.random.randint(-10, 11, size=2).tolist()) \\\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageOps.py\u001b[0m in \u001b[0;36minvert\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mlut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mlut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_lut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}